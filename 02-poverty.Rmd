---
output:
  pdf_document: default
  html_document: default
---
# Poverty Indices {#poverty}

Poverty has been a topic of conversation throughout human history. As @ravallion2016 points out, Aristotle and Confucius discussed ideas about poverty. In fact, Aristotle's ideas influenced Thomas Aquinas, one of the pillars of Western philosophy. Since then, societies changed, modifying the theories of justice underlying the idea of poverty.

As the concept and the ethics towards poverty change, so does its measurement. From basic measures like the headcount rate to more complex metrics, such as the Foster-Greer-Thorbecke (FGT) index, poverty measurement has evolved. Nowadays, poverty estimates are calculated using household surveys and censuses [@deaton1997]. However, only recently have the aspects of statistical inference combining such measures and complex sample survey designs been explored (for more discussion on this topic, see @deville1999, @berger2003, @bhat2007, and @osier2009).  These advances became even more important given the recent efforts in poverty mapping, an analytical method that combines poverty analysis and small area estimation, like @elbers2003 and @bedi2007.

This chapter shows how poverty estimates and their sampling errors can be estimated using simple commands from the `convey` package.

## At Risk of Poverty Threshold (svyarpt)

```{r eval=FALSE}
✔️ commonly used by statistical agencies in the european union working group on statistics on income & living conditions (eurostat)
✔️ not tied to the inflation rate nor to a basket of goods or consumable products
✔️ generic calculation that can be broadly applied to different nations or regions
✔️ easy to understand: defaults to 60% of median income
❌ the 60% of median income used in ARPT might appear arbitrary for non-EU analyses
❌ does not account for the intensity/severity of poverty
❌ not really a poverty measure, but an estimated poverty threshold/poverty line
```

The at-risk-of-poverty threshold (ARPT) is a measure used to define the people whose incomes imply a low standard of living in comparison to the general living standards.  Even though some people are not below the effective poverty line, those below the ARPT can be considered "almost deprived".

This measure is defined as $0.6$ times the median income for the entire population:

$$
arpt = 0.6 \times median(y),
$$
where $y$ is the income variable and `median` is estimated for the whole population. The details of the linearization of the ARPT are discussed by @deville1999 and @osier2009.

---

### Replication Example

The R `vardpoor` package [@vardpoor], created by researchers at the Central Statistical Bureau of Latvia, includes an ARPT coefficient calculation using the ultimate cluster method.  The example below reproduces those statistics.

Load and prepare the same data set:

```{r}
# load the convey package
library(convey)

# load the survey library
library(survey)

# load the vardpoor library
library(vardpoor)

# load the laeken library
library(laeken)

# load the synthetic EU statistics on income & living conditions
data(eusilc)

# make all column names lowercase
names(eusilc) <- tolower(names(eusilc))

# add a column with the row number
dati <- data.table::data.table(IDd = 1:nrow(eusilc), eusilc)

# calculate the arpt coefficient
# using the R vardpoor library
varpoord_arpt_calculation <-
  varpoord(
    # analysis variable
    Y = "eqincome",
    
    # weights variable
    w_final = "rb050",
    
    # row number variable
    ID_level1 = "IDd",
    
    # row number variable
    ID_level2 = "IDd",
    
    # strata variable
    H = "db040",
    
    N_h = NULL ,
    
    # clustering variable
    PSU = "rb030",
    
    # data.table
    dataset = dati,
    
    # arpt coefficient function
    type = "linarpt",
    
    # get linearized variable
    outp_lin = TRUE
  )


# construct a survey.design
# using our recommended setup
des_eusilc <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc
  )

# immediately run the convey_prep function on it
des_eusilc <- convey_prep(des_eusilc)

# coefficients do match
varpoord_arpt_calculation$all_result$value
coef(svyarpt( ~ eqincome , des_eusilc))

# linearized variables do match
# vardpoor
lin_arpt_varpoord <- varpoord_arpt_calculation$lin_out$lin_arpt
# convey
lin_arpt_convey <- attr(svyarpt( ~ eqincome , des_eusilc), "lin")

# check equality
all.equal(lin_arpt_varpoord, lin_arpt_convey)

# variances do not match exactly
attr(svyarpt( ~ eqincome , des_eusilc) , 'var')
varpoord_arpt_calculation$all_result$var

# standard errors do not match exactly
varpoord_arpt_calculation$all_result$se
SE(svyarpt( ~ eqincome , des_eusilc))
```

The variance estimate is computed by using the approximation defined in \@ref(var), while the linearized variable $z$ is defined by \@ref(lin). The functions `convey::svyarpt` and `vardpoor::linarpt` produce the same linearized variable $z$.

However, the measures of uncertainty do not line up, because `library(vardpoor)` defaults to an ultimate cluster method that can be replicated with an alternative setup of the `survey.design` object.

```{r}
# within each strata, sum up the weights
cluster_sums <-
  aggregate(eusilc$rb050 , list(eusilc$db040) , sum)

# name the within-strata sums of weights the `cluster_sum`
names(cluster_sums) <- c("db040" , "cluster_sum")

# merge this column back onto the data.frame
eusilc <- merge(eusilc , cluster_sums)

# construct a survey.design
# with the fpc using the cluster sum
des_eusilc_ultimate_cluster <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc ,
    fpc = ~ cluster_sum
  )

# again, immediately run the convey_prep function on the `survey.design`
des_eusilc_ultimate_cluster <-
  convey_prep(des_eusilc_ultimate_cluster)



# matches
stopifnot(all.equal(
  attr(svyarpt( ~ eqincome , des_eusilc_ultimate_cluster) , 'var')[1] ,
  varpoord_arpt_calculation$all_result$var
))

# matches
stopifnot(all.equal(varpoord_arpt_calculation$all_result$se ,
                    SE(
                      svyarpt( ~ eqincome , des_eusilc_ultimate_cluster)
                    )[1]))

```

For additional usage examples of `svyarpt`, type `?convey::svyarpt` in the R console.









### Real World Examples


This section displays example results using nationally-representative surveys from both the United States and Brazil.  We present a variety of surveys, levels of analysis, and subpopulation breakouts to provide users with a point of reference for the range of plausible values of the `svyarpt` function.

#### CPS-ASEC Household Income

```{r}
svyarpt(~ htotval , cps_household_design)

svyby(~ htotval , ~ sex , cps_household_design , svyarpt)
```

#### CPS-ASEC Family Income

```{r}
svyarpt(~ ftotval , cps_family_design)

svyby(~ ftotval , ~ sex , cps_family_design , svyarpt)
```

#### CPS-ASEC Worker Earnings

```{r}
svyarpt(~ ptotval , cps_ftfy_worker_design)

svyby(~ ptotval , ~ sex , cps_ftfy_worker_design , svyarpt)

svyarpt(~ pearnval , cps_ftfy_worker_design)

svyby(~ pearnval , ~ sex , cps_ftfy_worker_design , svyarpt)
```


#### PNAD-Contínua Per Capita Income

```{r}
svyarpt( ~ deflated_per_capita_income , pnadc_design , na.rm = TRUE)

svyby(~ deflated_per_capita_income ,
      ~ sex ,
      pnadc_design ,
      svyarpt ,
      na.rm = TRUE)
```

#### PNAD-Contínua Worker Earnings

```{r}
svyarpt( ~ deflated_labor_income , pnadc_design , na.rm = TRUE)

svyby( ~ deflated_labor_income , ~ sex , pnadc_design , svyarpt , na.rm = TRUE)
```




#### SCF Family Net Worth

```{r}
scf_MIcombine(with(scf_design , svyarpt( ~ networth)))

scf_MIcombine(with(scf_design , svyby( ~ networth, ~ hhsex , svyarpt)))
```

#### SCF Family Income

```{r}
scf_MIcombine(with(scf_design , svyarpt( ~ income)))

scf_MIcombine(with(scf_design , svyby( ~ income, ~ hhsex , svyarpt)))
```






## At Risk of Poverty Ratio (svyarpr)

```{r eval=FALSE}
✔️ EU standard like ARPT, easy to understand, interpret, and implement
✔️ proportion of individuals below ARPT -- a "companion" function that uses svyarpt() internally
✔️ measure is easy to understand
❌ does not account for the intensity or inequality among the poor
❌ not very common outside of the EU
❌ just another name for the `svyfgt( g = 0 , thresh = "relq" )`
```

The at-risk-of-poverty rate (ARPR) is the share of persons with an income below the at-risk-of-poverty threshold (ARPT). The logic behind this measure is that although most people below the ARPT cannot be considered "poor", they are the ones most vulnerable to becoming poor in the event of a negative economic phenomenon like a recession.

The ARPR is a composite estimate, taking into account both the sampling error in the proportion itself and that in the ARPT estimate. The details of the linearization of the ARPR are discussed by @deville1999 and @osier2009.

---

### Replication Example

The R `vardpoor` package [@vardpoor], created by researchers at the Central Statistical Bureau of Latvia, includes a ARPR coefficient calculation using the ultimate cluster method.  The example below reproduces those statistics.

Load and prepare the same data set:

```{r}
# load the convey package
library(convey)

# load the survey library
library(survey)

# load the vardpoor library
library(vardpoor)

# load the vardpoor library
library(laeken)

# load the synthetic EU statistics on income & living conditions
data(eusilc)

# make all column names lowercase
names(eusilc) <- tolower(names(eusilc))

# add a column with the row number
dati <- data.table::data.table(IDd = 1:nrow(eusilc), eusilc)

# calculate the arpr coefficient
# using the R vardpoor library
varpoord_arpr_calculation <-
  varpoord(
    # analysis variable
    Y = "eqincome",
    
    # weights variable
    w_final = "rb050",
    
    # row number variable
    ID_level1 = "IDd",
    
    # row number variable
    ID_level2 = "IDd",
    
    # strata variable
    H = "db040",
    
    N_h = NULL ,
    
    # clustering variable
    PSU = "rb030",
    
    # data.table
    dataset = dati,
    
    # arpr coefficient function
    type = "linarpr",
    
    # get linearized variable
    outp_lin = TRUE
    
  )


# construct a survey.design
# using our recommended setup
des_eusilc <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc
  )

# immediately run the convey_prep function on it
des_eusilc <- convey_prep(des_eusilc)

# coefficients do match
varpoord_arpr_calculation$all_result$value
coef(svyarpr( ~ eqincome , des_eusilc)) * 100

# linearized variables do not match
# because Fprime is the derivative wrt
# to the estimated threshold, not the estimated quantile
# for more details, see
# https://github.com/ajdamico/convey/issues/372#issuecomment-1656264143
#
# vardpoor
lin_arpr_varpoord <- varpoord_arpr_calculation$lin_out$lin_arpr
# convey
lin_arpr_convey <- attr(svyarpr( ~ eqincome , des_eusilc), "lin")

# check equality
all.equal(lin_arpr_varpoord, 100 * lin_arpr_convey)



# variances do not match exactly
attr(svyarpr( ~ eqincome , des_eusilc) , 'var') * 10000
varpoord_arpr_calculation$all_result$var

# standard errors do not match exactly
varpoord_arpr_calculation$all_result$se
SE(svyarpr( ~ eqincome , des_eusilc)) * 100
```
The variance estimate is computed by using the approximation defined in \@ref(var), while the linearized variable $z$ is defined by \@ref(lin). The functions `convey::svyarpr` and `vardpoor::linarpr` produce the same linearized variable $z$.

However, the measures of uncertainty do not line up. One of the reasons is that `library(vardpoor)` defaults to an ultimate cluster method that can be replicated with an alternative setup of the `survey.design` object.

```{r}
# within each strata, sum up the weights
cluster_sums <-
  aggregate(eusilc$rb050 , list(eusilc$db040) , sum)

# name the within-strata sums of weights the `cluster_sum`
names(cluster_sums) <- c("db040" , "cluster_sum")

# merge this column back onto the data.frame
eusilc <- merge(eusilc , cluster_sums)

# construct a survey.design
# with the fpc using the cluster sum
des_eusilc_ultimate_cluster <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc ,
    fpc = ~ cluster_sum
  )

# again, immediately run the convey_prep function on the `survey.design`
des_eusilc_ultimate_cluster <-
  convey_prep(des_eusilc_ultimate_cluster)

# does not match
attr(svyarpr( ~ eqincome , des_eusilc_ultimate_cluster) , 'var') * 10000
varpoord_arpr_calculation$all_result$var

# does not match
varpoord_arpr_calculation$all_result$se
SE(svyarpr( ~ eqincome , des_eusilc_ultimate_cluster)) * 100
```

Still, there is a difference in the estimates. This is discussed in detail in [this issue](https://github.com/ajdamico/convey/issues/372). 
In order to still provide additional examples for our code, we proceed with a Monte Carlo experiment. 

Using the `eusilcP` data from the `simPop` package [@R-simPop], we can compute the actual value of the at risk of poverty rate for that population:

```{r}
# load libraries
library(sampling)
library(survey)
library(convey)
library(parallel)

# load pseudo population data
data("eusilcP" , package = "simPop")

# compute population median
q50.pop <-
  convey:::computeQuantiles(eusilcP$eqIncome , rep(1 , length(eusilcP$eqIncome)) , .50)

# compute population poverty threshold
# as 60% of the median
thresh.pop <- .60 * q50.pop

# compute population at risk of poverty rate
(theta.pop <-
    mean(eusilcP$eqIncome <= thresh.pop , na.rm = TRUE))

```

Now, to study the distribution of the estimator under a particular sampling design, we select 5000 samples under one-stage cluster sampling of 100 households using the `cluster` function from the `sampling` package [@R-sampling], and use the `svyarpr` function to estimate the ARPR for each of those samples:

```{r}
# define the number of monte carlo replicates
mc.rep <- 5000L

# simulation function
arpr_sim_fun <- function(this.iter) {
  
  set.seed(this.iter)
  
  
  library(survey)
  library(convey)
  library(sampling)
  
  # load pseudo population data
  data("eusilcP" , package = "simPop")
  
  # compute size-like variable for PPS sampling design
  eusilcP$aux <-
    log(ifelse(eusilcP$eqIncome >= 1000 , eusilcP$eqIncome , 1000))
  
  
  # select sample
  tt <-
    sampling::cluster(
      data = eusilcP[sample.int(nrow(eusilcP) , nrow(eusilcP) , replace = FALSE) , ] ,
      clustername = "hid" ,
      size = 1000L ,
      method = "systematic" ,
      pik = eusilcP$aux
    )
  
  # collect data
  this.sample <- getdata(eusilcP , tt)
  
  # create survey design object
  this.desobj <-
    svydesign(
      ids = ~ hid ,
      probs = ~ Prob ,
      data = this.sample ,
      nest = FALSE
    )
  
  # prepare for convey functions
  this.desobj <- convey_prep(this.desobj)
  
  # compute estimates
  svyarpr( ~ eqIncome , this.desobj)
  
}

# run replications
cl <- makeCluster(detectCores() - 1)

arpr.estimate.list <-
  clusterApply(cl, seq_len(mc.rep) , arpr_sim_fun)

stopCluster(cl)
```

Then, we evaluate the Percentage Relative Bias (PRB) of the ARPR estimator. Under this scenario, the PRB of the ARPR estimator is -0.19830%.

```{r}
# estimate the expected value of the ARPR estimator
# using the average of the estimates
(theta.exp <- mean(sapply(arpr.estimate.list , coef)))

# estimate the percentage relative bias
(percentage_relative_bias_arpr <- 100 * (theta.exp / theta.pop - 1))

stopifnot(round(percentage_relative_bias_arpr , 4) == -0.19830)

```

For the variance estimator, we have:

```{r}
# estimate the variance of the ARPR estimator
# using the empirical variance of the estimates
(vartheta.popest <- var(sapply(arpr.estimate.list , coef)))

# estimate the expected value of the ARPR variance estimator
# using the (estimated) expected value of the variance estimates
(vartheta.exp <- mean(sapply(arpr.estimate.list , vcov)))

# estimate the percentage relative bias of the variance estimator
( percentage_relative_bias_variance <- 100 *  (vartheta.exp / vartheta.popest - 1) )

stopifnot( round( percentage_relative_bias_variance , 4 ) == 3.1720 ) 
```

Under this scenario, the PRB of the ARPR variance estimator is 3.1720%.

Our simulation shows that the Bias Ratio of this estimator is approximately 2%:

```{r}
# compute bias ratio
100 * abs( theta.exp - theta.pop ) / sqrt( vartheta.popest )
```

\noindent if the normal approximation holds, a small bias ratio still allows for approximately valid estimates of the confidence intervals.

Next, we evaluate the Percentage Coverage Rate (PCR). In theory, under repeated sampling, the estimated 95% CIs should cover the population parameter approximately 95% of the time. We can evaluate that using:

```{r}
# estimate confidence intervals of the ARPR
# for each of the samples
est.coverage <-
  sapply(arpr.estimate.list, function(this.stat)
    confint(this.stat)[, 1] <= theta.pop &
      confint(this.stat)[, 2] >= theta.pop)

# evaluate empirical coverage
(empirical_coverage <- mean(est.coverage))

stopifnot(round(empirical_coverage , 2) == 0.95)
```
Our coverages are not too far from the nominal coverage level of 95%.

For additional usage examples of `svyarpr`, type `?convey::svyarpr` in the R console.

## Relative Median Income Ratio (svyrmir)

```{r eval=FALSE}
✔️ mainly useful for studies of the income of the elderly following EU definitions
✔️ a ratio of medians
✔️ less sensitive to outliers
❌ solely a measure of medians and does not fully account for the income distribution
❌ not very common outside of the EU
❌ hard to interpret
❌ not (necessarily) a dependency measure
❌ not an inequality measure (fails the Pigou-Dalton principle)
❌ not (exactly) a poverty measure (fails the poverty-focus axiom)
```

The relative median income ratio (RMIR) is the ratio of the median income of people aged above a value (65) to the median of people aged below the same value. In mathematical terms,

$$
rmir = \frac{median\{y_i; age_i >65 \}}{median\{y_i; age_i \leq 65 \}}.
$$

The details of the linearization of the RMIR and are discussed by @deville1999 and @osier2009. 

---

### Replication Example

The R `vardpoor` package [@vardpoor], created by researchers at the Central Statistical Bureau of Latvia, includes a RMIR coefficient calculation using the ultimate cluster method.  The example below reproduces those statistics.

Load and prepare the same data set:

```{r}
# load the convey package
library(convey)

# load the survey library
library(survey)

# load the vardpoor library
library(vardpoor)

# load the vardpoor library
library(laeken)

# load the synthetic EU statistics on income & living conditions
data(eusilc)

# make all column names lowercase
names(eusilc) <- tolower(names(eusilc))

# add a column with the row number
dati <- data.table::data.table(IDd = 1:nrow(eusilc), eusilc)

# calculate the rmir coefficient
# using the R vardpoor library
varpoord_rmir_calculation <-
  varpoord(
    # analysis variable
    Y = "eqincome",
    
    # weights variable
    w_final = "rb050",
    
    # row number variable
    ID_level1 = "IDd",
    
    # row number variable
    ID_level2 = "IDd",
    
    # strata variable
    H = "db040",
    
    N_h = NULL ,
    
    # clustering variable
    PSU = "rb030",
    
    # data.table
    dataset = dati,
    
    # age variable
    age = "age",
    
    # rmir coefficient function
    type = "linrmir",
    
    # get linearized variable
    outp_lin = TRUE
    
  )



# construct a survey.design
# using our recommended setup
des_eusilc <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc
  )

# immediately run the convey_prep function on it
des_eusilc <- convey_prep(des_eusilc)

# coefficients do match
varpoord_rmir_calculation$all_result$value
coef(svyrmir( ~ eqincome , des_eusilc, age = ~ age))

# linearized variables do match
# vardpoor
lin_rmir_varpoord <- varpoord_rmir_calculation$lin_out$lin_rmir
# convey
lin_rmir_convey <-
  attr(svyrmir( ~ eqincome , des_eusilc, age = ~ age), "lin")

# check equality
all.equal(lin_rmir_varpoord, lin_rmir_convey[, 1])

# variances do not match exactly
attr(svyrmir( ~ eqincome , des_eusilc, age = ~ age) , 'var')
varpoord_rmir_calculation$all_result$var

# standard errors do not match exactly
varpoord_rmir_calculation$all_result$se
SE(svyrmir( ~ eqincome , des_eusilc , age = ~ age)) 
```

The variance estimate is computed by using the approximation defined in \@ref(var), while the linearized variable $z$ is defined by \@ref(lin). The functions `convey::svyrmir` and `vardpoor::linrmir` produce the same linearized variable $z$.

However, the measures of uncertainty do not line up, because `library(vardpoor)` defaults to an ultimate cluster method that can be replicated with an alternative setup of the `survey.design` object.

```{r}
# within each strata, sum up the weights
cluster_sums <-
  aggregate(eusilc$rb050 , list(eusilc$db040) , sum)

# name the within-strata sums of weights the `cluster_sum`
names(cluster_sums) <- c("db040" , "cluster_sum")

# merge this column back onto the data.frame
eusilc <- merge(eusilc , cluster_sums)

# construct a survey.design
# with the fpc using the cluster sum
des_eusilc_ultimate_cluster <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc ,
    fpc = ~ cluster_sum
  )

# again, immediately run the convey_prep function on the `survey.design`
des_eusilc_ultimate_cluster <-
  convey_prep(des_eusilc_ultimate_cluster)

# matches
stopifnot(all.equal(
  attr(
    svyrmir(~ eqincome , des_eusilc_ultimate_cluster , age = ~ age) ,
    'var'
  )[1],
  varpoord_rmir_calculation$all_result$var
))


# matches
stopifnot(all.equal(SE(
  svyrmir(~ eqincome , des_eusilc_ultimate_cluster, age = ~ age)
)[1], varpoord_rmir_calculation$all_result$se))
```

For additional usage examples of `svyrmir`, type `?convey::svyrmir` in the R console.

## Relative Median Poverty Gap (svyrmpg)

```{r eval=FALSE}
✔️ how poor are those below the ARPT?
✔️ median poverty gap expressed as a percentage of the threshold
✔️ useful for understanding the depth of poverty
❌ not common outside of the EU
❌ not immediately interpretable in terms of income
```

The relative median poverty gap (RMPG) is the relative difference between the median income of people having income below the ARPT and the ARPT itself:

$$
rmpg = \frac{median\{y_i, y_i<arpt\}-arpt}{arpt}
$$
The details of the linearization of the RMPG are discussed by @deville1999 and @osier2009.

---

### Replication Example

The R `vardpoor` package [@vardpoor], created by researchers at the Central Statistical Bureau of Latvia, includes a RMPG coefficient calculation using the ultimate cluster method.  The example below reproduces those statistics.

Load and prepare the same data set:

```{r}
# load the convey package
library(convey)

# load the survey library
library(survey)

# load the vardpoor library
library(vardpoor)

# load the vardpoor library
library(laeken)

# load the synthetic EU statistics on income & living conditions
data(eusilc)

# make all column names lowercase
names(eusilc) <- tolower(names(eusilc))

# add a column with the row number
dati <- data.table::data.table(IDd = 1:nrow(eusilc), eusilc)

# calculate the rmpg coefficient
# using the R vardpoor library
varpoord_rmpg_calculation <-
  varpoord(
    # analysis variable
    Y = "eqincome",
    
    # weights variable
    w_final = "rb050",
    
    # row number variable
    ID_level1 = "IDd",
    
    # row number variable
    ID_level2 = "IDd",
    
    # strata variable
    H = "db040",
    
    N_h = NULL ,
    
    # clustering variable
    PSU = "rb030",
    
    # data.table
    dataset = dati,
    
    # rmpg coefficient function
    type = "linrmpg",
    
    # get linearized variable
    outp_lin = TRUE
    
  )



# construct a survey.design
# using our recommended setup
des_eusilc <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc
  )

# immediately run the convey_prep function on it
des_eusilc <- convey_prep(des_eusilc)

# coefficients do match
varpoord_rmpg_calculation$all_result$value
coef(svyrmpg( ~ eqincome , des_eusilc)) * 100

# linearized variables do match
# vardpoor
lin_rmpg_varpoord <- varpoord_rmpg_calculation$lin_out$lin_rmpg
# convey
lin_rmpg_convey <- attr(svyrmpg( ~ eqincome , des_eusilc), "lin")

# check equality
all.equal(lin_rmpg_varpoord, 100 * lin_rmpg_convey[, 1])

# variances do not match exactly
attr(svyrmpg( ~ eqincome , des_eusilc) , 'var') * 10000
varpoord_rmpg_calculation$all_result$var

# standard errors do not match exactly
varpoord_rmpg_calculation$all_result$se
SE(svyrmpg( ~ eqincome , des_eusilc)) * 100
```

The variance estimate is computed by using the approximation defined in \@ref(var), while the linearized variable $z$ is defined by \@ref(lin). The functions `convey::svyrmpg` and `vardpoor::linrmpg` produce the same linearized variable $z$.

However, the measures of uncertainty do not line up, because `library(vardpoor)` defaults to an ultimate cluster method that can be replicated with an alternative setup of the `survey.design` object.

```{r}
# within each strata, sum up the weights
cluster_sums <-
  aggregate(eusilc$rb050 , list(eusilc$db040) , sum)

# name the within-strata sums of weights the `cluster_sum`
names(cluster_sums) <- c("db040" , "cluster_sum")

# merge this column back onto the data.frame
eusilc <- merge(eusilc , cluster_sums)

# construct a survey.design
# with the fpc using the cluster sum
des_eusilc_ultimate_cluster <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc ,
    fpc = ~ cluster_sum
  )

# again, immediately run the convey_prep function on the `survey.design`
des_eusilc_ultimate_cluster <-
  convey_prep(des_eusilc_ultimate_cluster)



# matches
stopifnot(all.equal(
  attr(svyrmpg( ~ eqincome , des_eusilc_ultimate_cluster) , 'var')[1] * 10000 ,
  varpoord_rmpg_calculation$all_result$var
))

# matches
stopifnot(all.equal(SE(
  svyrmpg(~ eqincome , des_eusilc_ultimate_cluster)
)[1] * 100 ,
varpoord_rmpg_calculation$all_result$se))

```

For additional usage examples of `svyrmpg`, type `?convey::svyrmpg` in the R console.

## Median Income Below the At Risk of Poverty Threshold  (svypoormed)

```{r eval=FALSE}
✔️ median income among those below the threshold
✔️ useful for understanding the depth of poverty
✔️ related to the RMPG
❌ not very common outside the EU
❌ not immediately interpretable in terms of the poverty gap
```

Median income below the at-risk-of-poverty-threshold (POORMED) is median of incomes of people having the income below the ARPT:

$$
poormed = median\{y_i; y_i< arpt\}
$$
The details of the linearization of the POORMED are discussed by @deville1999 and @osier2009.

---

### Replication Example

The R `vardpoor` package [@vardpoor], created by researchers at the Central Statistical Bureau of Latvia, includes a POORMED coefficient calculation using the ultimate cluster method.  The example below reproduces those statistics.

Load and prepare the same data set:

```{r}
# load the convey package
library(convey)

# load the survey library
library(survey)

# load the vardpoor library
library(vardpoor)

# load the vardpoor library
library(laeken)

# load the synthetic EU statistics on income & living conditions
data(eusilc)

# make all column names lowercase
names(eusilc) <- tolower(names(eusilc))

# add a column with the row number
dati <- data.table::data.table(IDd = 1:nrow(eusilc), eusilc)

# calculate the poormed coefficient
# using the R vardpoor library
varpoord_poormed_calculation <-
  varpoord(
    # analysis variable
    Y = "eqincome",
    
    # weights variable
    w_final = "rb050",
    
    # row number variable
    ID_level1 = "IDd",
    
    # row number variable
    ID_level2 = "IDd",
    
    # strata variable
    H = "db040",
    
    N_h = NULL ,
    
    # clustering variable
    PSU = "rb030",
    
    # data.table
    dataset = dati,
    
    # poormed coefficient function
    type = "linpoormed",
    
    # get linearized variable
    outp_lin = TRUE
    
  )



# construct a survey.design
# using our recommended setup
des_eusilc <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc
  )

# immediately run the convey_prep function on it
des_eusilc <- convey_prep(des_eusilc)

# coefficients do match
varpoord_poormed_calculation$all_result$value
coef(svypoormed( ~ eqincome , des_eusilc))

# linearized variables do match
# vardpoor
lin_poormed_varpoord <-
  varpoord_poormed_calculation$lin_out$lin_poormed
# convey
lin_poormed_convey <-
  attr(svypoormed( ~ eqincome , des_eusilc), "lin")

# check equality
all.equal(lin_poormed_varpoord, lin_poormed_convey)

# variances do not match exactly
attr(svypoormed( ~ eqincome , des_eusilc) , 'var')
varpoord_poormed_calculation$all_result$var

# standard errors do not match exactly
varpoord_poormed_calculation$all_result$se
SE(svypoormed( ~ eqincome , des_eusilc))
```

The variance estimate is computed by using the approximation defined in \@ref(var), while the linearized variable $z$ is defined by \@ref(lin). The functions `convey::svypoormed` and `vardpoor::linpoormed` produce the same linearized variable $z$.

However, the measures of uncertainty do not line up, because `library(vardpoor)` defaults to an ultimate cluster method that can be replicated with an alternative setup of the `survey.design` object.

```{r}
# within each strata, sum up the weights
cluster_sums <-
  aggregate(eusilc$rb050 , list(eusilc$db040) , sum)

# name the within-strata sums of weights the `cluster_sum`
names(cluster_sums) <- c("db040" , "cluster_sum")

# merge this column back onto the data.frame
eusilc <- merge(eusilc , cluster_sums)

# construct a survey.design
# with the fpc using the cluster sum
des_eusilc_ultimate_cluster <-
  svydesign(
    ids = ~ rb030 ,
    strata = ~ db040 ,
    weights = ~ rb050 ,
    data = eusilc ,
    fpc = ~ cluster_sum
  )

# again, immediately run the convey_prep function on the `survey.design`
des_eusilc_ultimate_cluster <-
  convey_prep(des_eusilc_ultimate_cluster)


# matches
stopifnot(all.equal(
  attr(svypoormed(~ eqincome , des_eusilc_ultimate_cluster) , 'var')[1],
  varpoord_poormed_calculation$all_result$var
))

# matches
stopifnot(all.equal(
  SE(svypoormed(~ eqincome , des_eusilc_ultimate_cluster))[1],
  varpoord_poormed_calculation$all_result$se
))

```

For additional usage examples of `svypoormed`, type `?convey::svypoormed` in the R console.

## Foster-Greer-Thorbecke class (svyfgt, svyfgtdec)

```{r eval=FALSE}
✔️ used widely because encompasses interpretable measures
✔️ allows an arbitrary poverty threshold
✔️ can incorporate intensity and inequality among the poor
✔️ when `g >= 2`, measure can be decomposed in interpretable measures of extension, intensity and inequality in poverty
❌ scales are dependent on `g`
❌ becomes increasingly difficult to interpret as `g` parameter grows
❌ no component that allows for time to exit poverty, unlike the watts index
```

@foster1984 proposed a family of indicators to measure poverty.  This class of $FGT$ measures, can be defined as

\[
p=\frac{1}{N}\sum_{k\in U}h(y_{k},\theta ), 
\]

where

\[
h(y_{k},\theta )=\left[ \frac{(\theta -y_{k})}{\theta }\right] ^{\gamma
}\delta \left\{ y_{k}\leq \theta \right\} , 
\]

where: $\theta$ is the poverty threshold; $\delta$ the indicator function that assigns value $1$ if the condition $\{y_{k}\leq \theta \}$ is satisfied and $0$ otherwise, and $\gamma$ is a non-negative constant.

If $\gamma =0$, the FGT(0) equals the poverty headcount ratio, which accounts for the spread of poverty. If $\gamma =1$, FGT(1) is the mean of the normalized income shortfall of the poor. By doing so, the measure takes into account both the spread and the intensity of poverty. When $\gamma =2$, the relative weight of larger shortfalls increases even more, which yields a measure that accounts for poverty severity, i.e., the inequality among the poor. This way, a transfer from a poor person to an even poorer person would reduce the FGT(2).

Although @foster1984 already presented a decomposition for the FGT(2) index, @aristondo2010 provided a general formula that decomposes the FGT($\gamma$) for any $\gamma \geqslant 2$. Put simply, any such FGT($\gamma$) index can be seen as function of the headcount ratio, the average normalized income gap among the poor, and a generalized entropy index of the normalized income gaps among poor. In mathematical terms,

\[
FGT_\gamma = FGT_0 \cdot I^\gamma \cdot \big[ 1 + \big( \gamma^2 -\gamma \big) GEI_\gamma^* \big] , \text{ } \gamma \geq 2
\]

where $I$ is the average normalized income gap among the poor and $GEI_\gamma^*$ is a generalized entropy index of such income gaps among the poor.

This result is particularly useful, as one can attribute cross-sectional differences of a FGT index to differences in the spread, depth, and inequality of poverty.

The FGT poverty class and its decomposition is implemented in the `convey` library by the function `svyfgt` and `svyfgtdec`, respectively.
The argument `thresh_type` of this function defines the type of poverty threshold adopted.
There are three possible choices:

1. `abs` -- fixed and given by the argument thresh_value.
2. `relq` -- a proportion of a quantile fixed by the argument `proportion` and the quantile is defined by the argument `order`.
3. `relm` -- a proportion of the mean fixed the argument `proportion`.

The quantile and the mean involved in the definition of the threshold are estimated for the whole population. When $\gamma=0$ and $\theta= .6*MED$, this measure is equal to the indicator ARPR computed by the function  `svyarpr`. The linearization of the FGT(0) is presented in @berger2003.

Next, we give some examples of the function `svyfgt` to estimate the values of the FGT poverty index.

Consider first the poverty threshold fixed ($\gamma=0$) in the value $10000$. The headcount ratio (FGT0) is

```{r comment=NA}
svyfgt( ~ eqincome, des_eusilc, g = 0, abs_thresh = 10000)
```

The poverty gap ratio (FGT(1)) ($\gamma=1$) index for the poverty threshold fixed at the same value is

```{r comment=NA}
svyfgt( ~ eqincome, des_eusilc, g = 1, abs_thresh = 10000)
```

To estimate the FGT(0) with the poverty threshold fixed at $0.6* MED$, we first fix the argument `type_thresh="relq"` and then use the default values for `percent` and `order`:

```{r comment=NA}
svyfgt( ~ eqincome, des_eusilc, g = 0, type_thresh = "relq")
```
This matches the estimate obtained by:

```{r comment=NA}
svyarpr( ~ eqincome, design = des_eusilc, .5, .6)
```
To estimate the poverty gap ratio with the poverty threshold equal to $0.6*MEAN$, we use:

```{r comment=NA}
svyfgt( ~ eqincome, des_eusilc, g = 1, type_thresh = "relm")
```

---

### Replication Example

In July 2006, @jenkins2006 presented at the North American Stata Users' Group Meetings on the stata Atkinson Index command.  The example below reproduces those statistics.

In order to match the presentation's results using the `svyfgt` function from the `convey` library, the poverty threshold was considered absolute despite being directly estimated from the survey sample.  This effectively treats the variance of the estimated poverty threshold as zero; `svyfgt` does not account for the uncertainty of the poverty threshold when the level has been stated as absolute with the `abs_thresh=` parameter.  In general, we would instead recommend using either `relq` or `relm` in the `type_thresh=` parameter in order to account for the added uncertainty of the poverty threshold calculation.  This example serves only to show that `svyfgt` behaves properly as compared to other software.

Load and prepare the same data set:
```{r}
# load the convey package
library(convey)

# load the survey library
library(survey)

# load the foreign library
library(foreign)

# create a temporary file on the local disk
tf <- tempfile()

# store the location of the presentation file
presentation_zip <-
  "https://web.archive.org/web/20150928053959/http://repec.org/nasug2006/nasug2006_jenkins.zip"

# download jenkins' presentation to the temporary file
download.file(presentation_zip , tf , mode = 'wb')

# unzip the contents of the archive
presentation_files <- unzip(tf , exdir = tempdir())

# load the institute for fiscal studies' 1981, 1985, and 1991 data.frame objects
x81 <-
  read.dta(grep("ifs81" , presentation_files , value = TRUE))
x85 <-
  read.dta(grep("ifs85" , presentation_files , value = TRUE))
x91 <-
  read.dta(grep("ifs91" , presentation_files , value = TRUE))

# NOTE: we recommend using ?convey::svyarpt rather than this unweighted calculation #

# calculate 60% of the unweighted median income in 1981
unwtd_arpt81 <- quantile(x81$eybhc0 , 0.5) * .6

# calculate 60% of the unweighted median income in 1985
unwtd_arpt85 <- quantile(x85$eybhc0 , 0.5) * .6

# calculate 60% of the unweighted median income in 1991
unwtd_arpt91 <- quantile(x91$eybhc0 , 0.5) * .6

# stack each of these three years of data into a single data.frame
x <- rbind(x81 , x85 , x91)
```

Replicate the author's survey design statement from stata code..
```
. ge poor = (year==1981)*(x < $z_81)+(year==1985)*(x < $z_85)+(year==1991)*(x < $z_91)
. * account for clustering within HHs 
. svyset hrn [pweight = wgt]
```

.. into R code:

```{r}
# initiate a linearized survey design object
y <- svydesign( ~ hrn , data = x , weights = ~ wgt)

# immediately run the `convey_prep` function on the survey design
z <- convey_prep(y)
```

Replicate the author's headcount ratio results with stata..
```
. svy: mean poor if year == 1981
(running mean on estimation sample)

Survey: Mean estimation

Number of strata =       1          Number of obs    =    9772
Number of PSUs   =    7476          Population size  = 5.5e+07
                                    Design df        =    7475

--------------------------------------------------------------
             |             Linearized
             |       Mean   Std. Err.     [95% Conf. Interval]
-------------+------------------------------------------------
        poor |   .1410125   .0044859       .132219     .149806
--------------------------------------------------------------

. svy: mean poor if year == 1985
(running mean on estimation sample)

Survey: Mean estimation

Number of strata =       1          Number of obs    =    8991
Number of PSUs   =    6972          Population size  = 5.5e+07
                                    Design df        =    6971

--------------------------------------------------------------
             |             Linearized
             |       Mean   Std. Err.     [95% Conf. Interval]
-------------+------------------------------------------------
        poor |    .137645   .0046531      .1285235    .1467665
--------------------------------------------------------------

. svy: mean poor if year == 1991
(running mean on estimation sample)

Survey: Mean estimation

Number of strata =       1          Number of obs    =    6468
Number of PSUs   =    5254          Population size  = 5.6e+07
                                    Design df        =    5253

--------------------------------------------------------------
             |             Linearized
             |       Mean   Std. Err.     [95% Conf. Interval]
-------------+------------------------------------------------
        poor |   .2021312   .0062077      .1899615    .2143009
--------------------------------------------------------------
```

..using R code:

```{r}
headcount_81 <-
  svyfgt( ~ eybhc0 ,
          subset(z , year == 1981) ,
          g = 0 ,
          abs_thresh = unwtd_arpt81)

stopifnot(round(coef(headcount_81) , 5) == .14101)
stopifnot(round(SE(headcount_81) , 5) == .00449)

headcount_81_ci <-
  confint(headcount_81 , df = degf(subset(z , year == 1981)))

stopifnot(round(headcount_81_ci[1] , 5) == .13222)
stopifnot(round(headcount_81_ci[2] , 5) == .14981)

headcount_85 <-
  svyfgt(~ eybhc0 ,
         subset(z , year == 1985) ,
         g = 0 ,
         abs_thresh = unwtd_arpt85)


stopifnot(round(coef(headcount_85) , 5) == .13764)
stopifnot(round(SE(headcount_85) , 5) == .00465)

headcount_85_ci <-
  confint(headcount_85 , df = degf(subset(z , year == 1985)))

stopifnot(round(headcount_85_ci[1] , 5) == .12852)
stopifnot(round(headcount_85_ci[2] , 5) == .14677)

headcount_91 <-
  svyfgt(~ eybhc0 ,
         subset(z , year == 1991) ,
         g = 0 ,
         abs_thresh = unwtd_arpt91)


stopifnot(round(coef(headcount_91) , 5) == .20213)
stopifnot(round(SE(headcount_91) , 5) == .00621)

headcount_91_ci <-
  confint(headcount_91 , df = degf(subset(z , year == 1991)))

stopifnot(round(headcount_91_ci[1] , 5) == .18996)
stopifnot(round(headcount_91_ci[2] , 5) == .21430)
```

Confirm this replication applies for the normalized poverty gap as well, comparing stata code..
```
. ge ngap = poor*($z_81- x)/$z_81 if year == 1981

. svy: mean ngap if year == 1981
(running mean on estimation sample)

Survey: Mean estimation

Number of strata =       1          Number of obs    =    9772
Number of PSUs   =    7476          Population size  = 5.5e+07
                                    Design df        =    7475

--------------------------------------------------------------
             |             Linearized
             |       Mean   Std. Err.     [95% Conf. Interval]
-------------+------------------------------------------------
        ngap |   .0271577   .0013502      .0245109    .0298044
--------------------------------------------------------------
```

..to R code:

```{r}
norm_pov_81 <-
  svyfgt( ~ eybhc0 ,
          subset(z , year == 1981) ,
          g = 1 ,
          abs_thresh = unwtd_arpt81)


stopifnot(round(coef(norm_pov_81) , 5) == .02716)
stopifnot(round(SE(norm_pov_81) , 5) == .00135)

norm_pov_81_ci <-
  confint(norm_pov_81 , df = degf(subset(z , year == 1981)))

stopifnot(round(norm_pov_81_ci[1] , 5) == .02451)
stopifnot(round(norm_pov_81_ci[2] , 5) == .02980)
```

### Monte Carlo Simulation

To provide a example for our code, we proceed with a Monte Carlo experiment. 
Using the `eusilcP` data from the `simPop` package [@R-simPop], we can compute the actual values of the FGT components for that population:

```{r}
# load libraries
library(sampling)
library(survey)
library(convey)
library(parallel)

# load pseudo population data
data("eusilcP" , package = "simPop")

# compute population value of the FGT(2) index decomposition
inc.all <- eusilcP$eqIncome
gfun <- function(y , thresh)
  (thresh - y) / thresh
hfun <-
  function(y , thresh , g)
    (((thresh - y) / thresh) ^ g) * (y <= thresh)
fgt2 <- mean(hfun(inc.all , 10000 , 2) , na.rm = TRUE)
fgt1 <- mean(hfun(inc.all , 10000 , 1) , na.rm = TRUE)
fgt0 <- mean(hfun(inc.all , 10000 , 0) , na.rm = TRUE)
igr.fgt <-
  mean(hfun(inc.all , 10000 , 1)[inc.all <= 10000] , na.rm = TRUE)
gei.poor <-
  convey:::CalcGEI(gfun(inc.all , 10000) , ifelse(inc.all < 10000 , 1 , 0) , 2)
theta.pop <-
  c(
    "fgt2" = fgt2 ,
    "fgt0" =  fgt0 ,
    "fgt1" =  fgt1 ,
    "igr" = igr.fgt ,
    "gei(poor;epsilon=2)" =  gei.poor
  )
theta.pop

```

Now, to study the distribution of the estimator under a particular sampling design, we select 5000 samples under one-stage cluster sampling of 100 households using the `cluster` function from the `sampling` package [@R-sampling], and use the `svyfgtdec` function to estimate the FGT components for each of those samples:

```{r}

# define the number of monte carlo replicates
mc.rep <- 5000L



# simulation function
fgtdec_sim_fun <- function(this.iter) {
  set.seed(this.iter)
  
  library(sampling)
  library(survey)
  library(convey)
  
  # load pseudo population data
  data("eusilcP" , package = "simPop")
  
  
  # compute size-like variable for PPS sampling design
  eusilcP$aux <-
    log(ifelse(eusilcP$eqIncome >= 1000 , eusilcP$eqIncome , 1000))
  
  
  # select sample
  tt <-
    sampling::cluster(
      data = eusilcP[sample.int(nrow(eusilcP) , nrow(eusilcP) , replace = FALSE) , ] ,
      clustername = "hid" ,
      size = 1000L ,
      method = "systematic" ,
      pik = eusilcP$aux
    )
  
  # collect data
  this.sample <- getdata(eusilcP , tt)
  
  # create survey design object
  this.desobj <-
    svydesign(
      ids = ~ hid ,
      probs = ~ Prob ,
      data = this.sample ,
      nest = FALSE
    )
  
  # prepare for convey functions
  this.desobj <- convey_prep(this.desobj)
  
  # compute estimates
  svyfgtdec( ~ eqIncome , this.desobj , abs_thresh = 10000 , g = 2)
  
}

# run replications
cl <- makeCluster(detectCores() - 1)

fgtdec.estimate.list <-
  clusterApply(cl, seq_len(mc.rep) , fgtdec_sim_fun)

stopCluster(cl)
```

The PRB of each component is estimated using the code below. Notice that PRBs are relatively small, with absolute values below 1%, with the largest bias in the GEI index component.

```{r}
# repeat true population values
(theta.pop)

# estimate the expected values of the components estimators
# using the average of the estimates
(theta.exp <- rowMeans(sapply(fgtdec.estimate.list , coef)))

# estimate the percentage relative bias
(percentage_relative_bias <- 100 * (theta.exp / theta.pop - 1))

stopifnot(abs(percentage_relative_bias) < 1) 
```

For the variance estimators, we estimate the PRB using the code below. 
Note that the bias is still relatively small, with absolute values of the PRB below 5%.

```{r}
# estimate the variance of the components estimators
# using the empirical variance of the estimates
(vartheta.popest <-
   diag(var(t(
     sapply(fgtdec.estimate.list , coef)
   ))))

# estimate the expected value of the Watts index variance estimator
# using the (estimated) expected value of the variance estimates
(vartheta.exp <-
    rowMeans(sapply(fgtdec.estimate.list , function(z)
      diag(vcov(
        z
      )))))

# estimate the percentage relative bias of the variance estimators
(percentage_relative_bias_variance <- 100 *  (vartheta.exp / vartheta.popest - 1))

stopifnot(abs(percentage_relative_bias_variance) < 5)
```

Regarding the MSE of the decomposition estimator, the squared bias accounts for less than 0.1% of the MSE.
This means that, with a good estimate of the variance, we should be able to have a good approximation for the MSE.

```{r}
# estimate MSE
theta.bias2 <- (theta.exp - theta.pop) ^ 2
(theta.mse <- theta.bias2 + vartheta.popest)

# squared bias component of the MSE
( squared_bias_over_mse <- 100 * (theta.bias2 / theta.mse) )

stopifnot( squared_bias_over_mse < 1 )
```

The CIs based on the normal approximation work reasonably well for all components. The code below shows that the coverage rates are close to the 95% nominal coverage rate.

```{r}
# estimate confidence intervals of the Watts index
# for each of the samples
est.coverage <-
  sapply(fgtdec.estimate.list, function(this.stat)
    confint(this.stat)[, 1] <= theta.pop &
      confint(this.stat)[, 2] >= theta.pop)

# evaluate empirical coverage
stopifnot( abs( rowMeans(est.coverage) - 0.95 ) < 0.015 )
```

For additional usage examples of `svyfgt` and `svyfgtdec`, type `?convey::svyfgt` or `?convey::svyfgtdec` in the R console.

## Watts poverty measure (svywatts, svywattsdec)

```{r eval=FALSE}
✔️ time to exit poverty interpretation: watts score divided by a growth rate
✔️ sensitive to intensity and inequality among the poor
✔️ can be decomposed into interpretable measures
❌ not defined for individuals with zero or negative incomes
❌ interpretation is not very straightforward
❌ less common than the FGT measures nowadays
```

The measure proposed in @watts1968 satisfies a number of desirable poverty measurement axioms and is known to be one of the first distribution-sensitive poverty measures, as noted by @haughton2009. It is defined as:

\[
Watts = \frac{1}{N} \sum_{i \in U} \log{ \bigg( \frac{y_i}{\theta} \bigg) \delta ( y_i \leqslant \theta) }.
\]

@murdoch1998 points out that the Watts poverty index can provide an estimate of the expected time to exit poverty. Given the expected growth rate of income per capita among the poor, $g$, the expected time taken to exit poverty $T_\theta$ would be:

\[
T_\theta = \frac{Watts}{g}.
\]

The Watts poverty index also has interesting decomposition properties. @blackburn1989 proposed a decomposition for the Watts poverty index, rewriting it in terms of the headcount ratio, the Watts poverty gap ratio and the mean log deviaton of poor incomes^[The mean log deviation (also known as Theil-L or Bourguignon-Theil index) is an inequality measure of the generalized entropy class. The family of generalized entropy indices is discussed in the next chapter.]. Mathematically,

\[
Watts = FGT_0 \big( I_w + L_* \big)
\]

where $I_w = \log(\theta/\mu_*)$ is the Watts poverty gap ratio^[ $\mu_*$ stands for the average income among the poor.] and $L_*$ is the mean log deviation of incomes among the poor. This can be estimated using the `svywattsdec` function.

This result can also be interpreted as a decomposition of the time taken to exit poverty, since

\[
\begin{aligned}
T_\theta &= \frac{Watts}{g} \\ 
&= \frac{FGT_0}{g} \big( I_w + L_* \big)
\end{aligned}
\]

As @murdoch1998 points out, if the income among the poor is equally distributed (i.e., $L_*=0$), the time taken to exit poverty is simply $FGT_0 I_w / g$. Therefore, $FGT_0 L_* / g$ can be seen as the additional time needed to exit poverty as a result of the inequality among the poor.

---

### Monte Carlo Simulation

To provide a example for our code, we proceed with a Monte Carlo experiment. 
Using the `eusilcP` data from the `simPop` package [@R-simPop], we can compute the actual value of the Watts index for that population:

```{r}
# load libraries
library(sampling)
library(survey)
library(convey)
library(parallel)

# load pseudo population data
data("eusilcP" , package = "simPop")

# compute population value of the Watts index decomposition
inc.pos <- eusilcP$eqIncome[eusilcP$eqIncome > 0]
(theta.pop <-
    mean(ifelse(inc.pos <= 10000 , log(10000 /  inc.pos) , 0) , na.rm = TRUE))

```

Now, to study the distribution of the estimator under a particular sampling design, we select 5000 samples under one-stage cluster sampling of 100 households using the `cluster` function from the `sampling` package [@R-sampling], and use the `svywatts` function to estimate the Watts index for each of those samples:

```{r}



# define the number of monte carlo replicates
mc.rep <- 5000L


# simulation function
watts_sim_fun <- function(this.iter) {
  set.seed(this.iter)
  
  library(survey)
  library(convey)
  library(sampling)
  
  
  # load pseudo population data
  data("eusilcP" , package = "simPop")
  
  # compute size-like variable for PPS sampling design
  eusilcP$aux <-
    log(ifelse(eusilcP$eqIncome >= 1000 , eusilcP$eqIncome , 1000))
  
  # select sample
  tt <-
    sampling::cluster(
      data = eusilcP[sample.int(nrow(eusilcP) , nrow(eusilcP) , replace = FALSE) ,] ,
      clustername = "hid" ,
      size = 1000L ,
      method = "systematic" ,
      pik = eusilcP$aux
    )
  
  # collect data
  this.sample <- getdata(eusilcP , tt)
  
  # create survey design object
  this.desobj <-
    svydesign(
      ids = ~ hid ,
      probs = ~ Prob ,
      data = this.sample ,
      nest = FALSE
    )
  
  # prepare for convey functions
  this.desobj <- convey_prep(this.desobj)
  
  # filter positive incomes
  this.desobj <- subset(this.desobj , eqIncome > 0)
  
  # compute estimates
  svywatts( ~ eqIncome , this.desobj , abs_thresh = 10000)
  
}

# run replications
cl <- makeCluster(detectCores() - 1)

watts.estimate.list <-
  clusterApply(cl, seq_len(mc.rep) , watts_sim_fun)

stopCluster(cl)

```

Then, we evaluate the Percentage Relative Bias (PRB) of the Watts index estimator. Under this scenario, the PRB of the Watts index estimator is 0.3772%.

```{r}
# estimate the expected value of the Watts index estimator
# using the average of the estimates
(theta.exp <- mean(sapply(watts.estimate.list , coef)))

# estimate the percentage relative bias
(percentage_relative_bias <- 100 * (theta.exp / theta.pop - 1) )

stopifnot( round( percentage_relative_bias , 4 ) == 0.3772)
```

For the variance estimator, we have:

```{r}
# estimate the variance of the Watts index estimator
# using the empirical variance of the estimates
(vartheta.popest <- var(sapply(watts.estimate.list , coef)))

# estimate the expected value of the Watts index variance estimator
# using the (estimated) expected value of the variance estimates
(vartheta.exp <- mean(sapply(watts.estimate.list , vcov)))

# estimate the percentage relative bias of the variance estimator
(percentage_relative_bias_variance <- 100 *  (vartheta.exp / vartheta.popest - 1))

stopifnot(round(percentage_relative_bias_variance, 4) == -0.6600)
```

Under this scenario, the PRB of the Watts index variance estimator is -0.6600%.

Our simulations show that the Squared Bias of this estimator accounts for less than 0.1% of its Mean Squared Error:

```{r}
theta.bias2 <- (theta.exp - theta.pop) ^ 2
theta.mse <- theta.bias2 + vartheta.popest
(squared_bias_over_mse <- 100 * (theta.bias2 / theta.mse))

stopifnot(squared_bias_over_mse < 0.1)
```

Next, we evaluate the Percentage Coverage Rate (PCR). In theory, under repeated sampling, the estimated 95% CIs should cover the population parameter approximately 95% of the time. We can evaluate that using:

```{r}
# estimate confidence intervals of the Watts index
# for each of the samples
est.coverage <-
  sapply(watts.estimate.list, function(this.stat)
    confint(this.stat)[, 1] <= theta.pop &
      confint(this.stat)[, 2] >= theta.pop)

# evaluate empirical coverage
(empirical_coverage <- mean(est.coverage))

stopifnot(abs(empirical_coverage  - 0.95) < 0.025)
```
Our coverages are not too far from the nominal coverage level of 95%.

For the Watts index decomposition, we start by computing the (true) population values of the components:

```{r}
# compute population value of the Watts index decomposition
inc.pos <- eusilcP$eqIncome[eusilcP$eqIncome > 0]
wdec1 <-
  mean(ifelse(inc.pos <= 10000 , log(10000 /  inc.pos) , 0) , na.rm = TRUE)
wdec2 <- mean(inc.pos <= 10000 , na.rm = TRUE)
mu.poor <- mean(inc.pos [inc.pos <= 10000])
wdec3 <- log(10000 / mu.poor)
wdec4 <-
  -mean(log(inc.pos[inc.pos <= 10000] / mu.poor) , na.rm = TRUE)
theta.pop <-
  c(
    "watts" = wdec1 ,
    "fgt0" =  wdec2 ,
    "watts pov. gap ratio" = wdec3 ,
    "theil(poor)" =  wdec4
  )
theta.pop
```

Then, using the same sampling strategy of the `svywatts`, we compute the `svywattsdec` for each sample:

```{r}
# simulation function
wattsdec_sim_fun <- function(this.iter) {
  
  set.seed(this.iter)
  
  library(survey)
  library(convey)
  library(sampling)
  
  
  # load pseudo population data
  data("eusilcP" , package = "simPop")
  
  # compute size-like variable for PPS sampling design
  eusilcP$aux <-
    log(ifelse(eusilcP$eqIncome >= 1000 , eusilcP$eqIncome , 1000))
  
  # select sample
  tt <-
    sampling::cluster(
      data = eusilcP[sample.int(nrow(eusilcP) , nrow(eusilcP) , replace = FALSE) , ] ,
      clustername = "hid" ,
      size = 1000L ,
      method = "systematic" ,
      pik = eusilcP$aux
    )
  
  # collect data
  this.sample <- getdata(eusilcP , tt)
  
  # create survey design object
  this.desobj <-
    svydesign(
      ids = ~ hid ,
      probs = ~ Prob ,
      data = this.sample ,
      nest = FALSE
    )
  
  # prepare for convey functions
  this.desobj <- convey_prep(this.desobj)
  
  # filter positive incomes
  this.desobj <- subset(this.desobj , eqIncome > 0)
  
  # compute estimates
  svywattsdec(~ eqIncome , this.desobj , abs_thresh = 10000)
  
}

# run replications
cl <- makeCluster(detectCores() - 1)

wattsdec.estimate.list <-
  clusterApply(cl, seq_len(mc.rep) , wattsdec_sim_fun)

stopCluster(cl)

```

The PRB of each component is estimated using the code below. Notice that PRBs are relatively small, with absolute values below 1%, with the largest bias in the Theil index component.

```{r}
# repeat true population values
(theta.pop)

# estimate the expected values of the components estimators
# using the average of the estimates
(theta.exp <- rowMeans(sapply(wattsdec.estimate.list , coef)))

# estimate the percentage relative bias
(percentage_relative_bias <- 100 * (theta.exp / theta.pop - 1))

stopifnot(abs(percentage_relative_bias) < 1)
```

For the variance estimators, we estimate the PRB using the code below. 
Note that the bias of the variance estimators is still relatively small, with absolute value of the Watts variance estimator's PRB below 1% and all four components variance estimators below 5%.

```{r}
# estimate the variance of the components estimators
# using the empirical variance of the estimates
(vartheta.popest <-
   diag(var(t(
     sapply(wattsdec.estimate.list , coef)
   ))))

# estimate the expected value of the Watts index variance estimator
# using the (estimated) expected value of the variance estimates
(vartheta.exp <-
    rowMeans(sapply(wattsdec.estimate.list , function(z)
      diag(vcov(
        z
      )))))

# estimate the percentage relative bias of the variance estimators
(percentage_relative_bias <-
    100 *  (vartheta.exp / vartheta.popest - 1))

stopifnot(abs(percentage_relative_bias[1]) < 1 & all(abs(percentage_relative_bias) < 5))
```

Regarding the MSE, the squared bias accounts for less than 0.1% of the MSE. 
This means that, with a good estimate of the variance, we should be able to have a good approximation for the MSE.

```{r}
# estimate MSE
theta.bias2 <- (theta.exp - theta.pop) ^ 2
(theta.mse <- theta.bias2 + vartheta.popest)

# squared bias component of the MSE
(squared_bias_over_mse <- 100 * (theta.bias2 / theta.mse))

stopifnot(squared_bias_over_mse < 0.1)

```

However, the CIs based on the normal approximation might not work very well for some components. The code below shows that coverage rate for the Theil index component differs significantly from the 95% nominal coverage rate.

```{r}
# estimate confidence intervals of the Watts index
# for each of the samples
est.coverage <-
  sapply(wattsdec.estimate.list, function(this.stat)
    confint(this.stat)[, 1] <= theta.pop &
      confint(this.stat)[, 2] >= theta.pop)

# evaluate empirical coverage
(empirical_coverage <- rowMeans(est.coverage))

stopifnot(abs(empirical_coverage - 0.95) < 0.15)
```

One of the reasons for this is that the sample might not be large enough for the CLT to hold. The distribution of the estimator shows substantial asymmetry, which would be a problem for the normal approximation.

```{r}
hist(
  sapply(wattsdec.estimate.list , coef)[4, ] ,
  main = "Histogram of Theil component estimator" ,
  xlim = c(0, .30) ,
  ylim = c(0 , 1500) ,
  xlab = "Estimate"
)
```


For additional usage examples of `svywatts` and `svywattsdec`, type `?convey::svywatts` or `?convey::svywattsdec` in the R console.
